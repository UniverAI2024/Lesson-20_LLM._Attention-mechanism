# Lesson-20_LLM._Attention-mechanism

Сегодня разберем принципы работы attention в задачах seq2seq.

Attention — это не нейронная сеть, но метод ее построения. С его помощи нейросетям показывают важную часть входного изображения или текста.  
Освоение материалов занятия позволит изучит более продвинутую технологию: механиз transformers
